{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS5002 Project 1 - Part 1\n",
    "# Author: Xiaoyuan Lu\n",
    "# NUID: 002034691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c699a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb675d55",
   "metadata": {},
   "source": [
    "## Section 1: Derivatives and Optimization\n",
    "\n",
    "This section introduces derivatives and their role in function optimization. It prepares the foundation for implementing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def deriv_f1(x):\n",
    "    return 2 * x\n",
    "\n",
    "def f2(x):\n",
    "    return x**2 - 2 * x + 3\n",
    "\n",
    "def deriv_f2(x):\n",
    "    return 2 * x - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8be57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, x0, alpha, epsilon, iter_max):\n",
    "    x = x0\n",
    "    iter_count = 0\n",
    "    while iter_count < iter_max:\n",
    "        x_new = x - alpha * df(x)\n",
    "        if abs(x_new - x) < epsilon:\n",
    "            break\n",
    "        x = x_new\n",
    "        iter_count += 1\n",
    "    return x, iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_opt(f, optimal_x, title):\n",
    "    x_vals = np.linspace(-5, 5, 400)\n",
    "    y_vals = f(x_vals)\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals, label='Function')\n",
    "    plt.scatter(optimal_x, f(optimal_x), color='red', label='Optimal x')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d07b9",
   "metadata": {},
   "source": [
    "## Section 2: Gradient Descent\n",
    "\n",
    "This section focuses on implementing the gradient descent algorithm and applying it to simple quadratic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fde592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_tests():\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.001\n",
    "    iter_max = 1000\n",
    "\n",
    "    print(\"\\nTesting with different x0:\")\n",
    "    for x0 in [3, -3]:\n",
    "        for f, df, name in [(f1, deriv_f1, \"f1\"), (f2, deriv_f2, \"f2\")]:\n",
    "            opt_x, iters = gradient_descent(f, df, x0, alpha, epsilon, iter_max)\n",
    "            print(f\"{name} with x0={x0}: Optimal x = {opt_x:.4f}, Iterations = {iters}\")\n",
    "\n",
    "    print(\"\\nTesting with different alpha:\")\n",
    "    for alpha in [1, 0.001, 0.0001]:\n",
    "        opt_x, iters = gradient_descent(f1, deriv_f1, 3, alpha, epsilon, iter_max)\n",
    "        print(f\"alpha={alpha}: Optimal x = {opt_x:.4f}, Iterations = {iters}\")\n",
    "\n",
    "    print(\"\\nTesting with different epsilon:\")\n",
    "    for epsilon in [0.1, 0.01, 0.0001]:\n",
    "        opt_x, iters = gradient_descent(f1, deriv_f1, 3, 0.1, epsilon, iter_max)\n",
    "        print(f\"epsilon={epsilon}: Optimal x = {opt_x:.4f}, Iterations = {iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b094f",
   "metadata": {},
   "source": [
    "### 2.1 Quadratic Functions\n",
    "\n",
    "We implement and test gradient descent on two quadratic functions: $f_1(x) = x^2$ and $f_2(x) = x^2 - 2x + 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    return np.sin(x) + np.cos(np.sqrt(2) * x)\n",
    "\n",
    "def deriv_f3(x):\n",
    "    return np.cos(x) - np.sqrt(2) * np.sin(np.sqrt(2) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f3():\n",
    "    x_vals = np.linspace(0, 10, 500)\n",
    "    y_vals = f3(x_vals)\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals, label='f3(x)')\n",
    "    plt.title('Function f3(x) = sin(x) + cos(sqrt(2)x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f3(x)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f422e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_f3_minima():\n",
    "    x0_values = [1, 4, 5, 7]\n",
    "    alpha = 0.1\n",
    "    epsilon = 0.0001\n",
    "    iter_max = 1000\n",
    "\n",
    "    x_vals = np.linspace(0, 10, 500)\n",
    "    y_vals = f3(x_vals)\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals, label='f3(x)')\n",
    "\n",
    "    for x0 in x0_values:\n",
    "        opt_x, _ = gradient_descent(f3, deriv_f3, x0, alpha, epsilon, iter_max)\n",
    "        plt.scatter(opt_x, f3(opt_x), label=f'x0={x0}')\n",
    "\n",
    "    plt.title('Gradient Descent on f3 with Different Starting Points')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f3(x)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb8525",
   "metadata": {},
   "source": [
    "## Section 3: Derivative Approximation for One Variable\n",
    "\n",
    "This section approximates derivatives numerically to simulate scenarios where analytical forms are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa766e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_approx(f, x0, alpha, epsilon, iter_max):\n",
    "    x = x0\n",
    "    iter_count = 0\n",
    "    while iter_count < iter_max:\n",
    "        x_new = x - alpha * approx_derivative(f, x)\n",
    "        if abs(x_new - x) < epsilon:\n",
    "            break\n",
    "        x = x_new\n",
    "        iter_count += 1\n",
    "    return x, iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_approximation():\n",
    "    for f, name in [(f1, \"f1\"), (f2, \"f2\")]:\n",
    "        print(f\"\\nTesting derivative approximation for {name}:\")\n",
    "        for x in [1, 2, 3]:\n",
    "            exact = deriv_f1(x) if name == \"f1\" else deriv_f2(x)\n",
    "            approx = approx_derivative(f, x)\n",
    "            print(f\"x={x}: exact = {exact:.4f}, approx = {approx:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149265d",
   "metadata": {},
   "source": [
    "## Section 4: Gradient Descent for Two Variables\n",
    "\n",
    "Here we generalize gradient descent to two-variable functions and visualize the optimization process in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_multi(x, y):\n",
    "    return x**2 + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49133e",
   "metadata": {},
   "source": [
    "## Section 5: Report Summary\n",
    "\n",
    "This section summarizes the structure, implementation, and findings from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_partial_deriv(f, x, y, var='x', h=1e-5):\n",
    "    if var == 'x':\n",
    "        return (f(x + h, y) - f(x, y)) / h\n",
    "    else:\n",
    "        return (f(x, y + h) - f(x, y)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134450c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(f, x0, y0, alpha, epsilon, iter_max):\n",
    "    x, y = x0, y0\n",
    "    for _ in range(iter_max):\n",
    "        dx = approx_partial_deriv(f, x, y, 'x')\n",
    "        dy = approx_partial_deriv(f, x, y, 'y')\n",
    "        x_new = x - alpha * dx\n",
    "        y_new = y - alpha * dy\n",
    "        if abs(x_new - x) < epsilon and abs(y_new - y) < epsilon:\n",
    "            break\n",
    "        x, y = x_new, y_new\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_result():\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    x_vals = y_vals = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x_vals, y_vals)\n",
    "    Z = f_multi(X, Y)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "\n",
    "    opt_x, opt_y = gradient_descent_2d(f_multi, 3, 3, 0.1, 0.001, 1000)\n",
    "    ax.scatter(opt_x, opt_y, f_multi(opt_x, opt_y), color='r', s=50)\n",
    "    ax.set_title(\"Gradient Descent on f(x, y) = x^2 + y^2\")\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('f(x, y)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d92ad",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27617bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Section 2.1: Gradient Descent on simple functions\n",
    "    opt1, _ = gradient_descent(f1, deriv_f1, 3, 0.1, 0.001, 1000)\n",
    "    plot_opt(f1, opt1, \"Gradient Descent on f1(x) = x^2\")\n",
    "\n",
    "    opt2, _ = gradient_descent(f2, deriv_f2, 3, 0.1, 0.001, 1000)\n",
    "    plot_opt(f2, opt2, \"Gradient Descent on f2(x) = x^2 - 2x + 3\")\n",
    "\n",
    "    # Section 2.1: Parameter tuning experiments\n",
    "    tuning_tests()\n",
    "\n",
    "    # Section 2.2: Complex function analysis\n",
    "    plot_f3()\n",
    "    test_f3_minima()\n",
    "\n",
    "    # Section 3: Numerical derivative testing\n",
    "    test_approximation()\n",
    "\n",
    "    # Section 4: Two-variable function optimization\n",
    "    plot_2d_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
